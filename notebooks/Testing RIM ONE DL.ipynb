{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc003fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s):\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    os.environ['PYTHONHASHSEED'] = str(s)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0993a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import equalize_adapthist\n",
    "from skimage.transform import warp_polar\n",
    "\n",
    "class CLAHE(torch.nn.Module):\n",
    "    def forward(self, img):\n",
    "        image = np.array(img, dtype=np.float64) / 255.0\n",
    "        image = equalize_adapthist(image)\n",
    "        image = (image*255).astype('uint8')\n",
    "\n",
    "        return image\n",
    "\n",
    "class POLAR(torch.nn.Module):\n",
    "    def polar(self,image):\n",
    "        return warp_polar(image, radius=(max(image.shape) // 2), multichannel=True)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        image = np.array(image, dtype=np.float64)\n",
    "        image = self.polar(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f297cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "split = \"test\"\n",
    "batch_size = 32\n",
    "num_workers = 32\n",
    "train_path = f\"PATH_TO_DATA/partitioned_by_hospital/training_set\" # path to dataset training set\n",
    "path = f\"PATH_TO_DATA/partitioned_by_hospital/{split}_set\"        # path to dataset folder\n",
    "output_dir = \"PATH_TO_SAVE_OUTPUTS\"                               # path to save checkpoints\n",
    "\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "            CLAHE(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomAffine(0,scale=(1.0,1.3))\n",
    "        ])\n",
    "transform = torchvision.transforms.Compose([\n",
    "            CLAHE(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256)\n",
    "        ])\n",
    "train_dataset = ImageFolder(train_path, transform=train_transform)\n",
    "num = int(np.floor(len(train_dataset) * 1))\n",
    "indices = np.random.choice(len(train_dataset), num, replace=False)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                  batch_size=batch_size, \n",
    "                  shuffle=True,\n",
    "                  num_workers=num_workers,\n",
    "              )\n",
    "test_dataset = ImageFolder(path, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                  batch_size=batch_size, \n",
    "                  shuffle=True,\n",
    "                  num_workers=num_workers,\n",
    "              )\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_labels = []\n",
    "for j in range(len(train_dataset)):\n",
    "    _labels.append(train_dataset[j][1])\n",
    "_labels = np.asarray(_labels)\n",
    "np.unique(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18daca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model_name = \"efficientnet_b0\"\n",
    "pretrained = True\n",
    "dropout = 0.2\n",
    "lr = 0.0005\n",
    "#momentum = 0.1\n",
    "epochs = 20\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=pretrained, num_classes=2, drop_rate=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350aaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"PATH_TO_CKPTS/rimonedl_1.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Best F1 {} from epoch {}\\n\".format(checkpoint[\"best_f1\"], checkpoint[\"epoch\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd683741",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "weight_referable = class_weight.compute_class_weight(class_weight='balanced', classes = np.unique(_labels), y=_labels).astype('float32')    \n",
    "weight_referable = np.array([weight_referable[0], weight_referable[1]])\n",
    "criterion = CrossEntropyLoss(weight=torch.from_numpy(weight_referable).to(device))\n",
    "print(weight_referable)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "epoch_resume = 0\n",
    "best_f1 = 0.0\n",
    "\n",
    "\n",
    "# Train\n",
    "if epoch_resume < epochs:\n",
    "    print(\"Resuming training\\n\")\n",
    "    for epoch in range(epoch_resume, epochs):\n",
    "        for split in ['Train']:\n",
    "            if split == \"Train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_total_loss = 0\n",
    "            labels = []\n",
    "            predictions = []\n",
    "            loader = train_loader if split == \"Train\" else val_loader\n",
    "            for batch_num, (inp, target) in enumerate(tqdm(loader)):\n",
    "                labels+=(target)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inp.to(device))\n",
    "                _, batch_prediction = torch.max(output, dim=1)\n",
    "                predictions += batch_prediction.detach().tolist()\n",
    "                batch_loss = criterion(output, (target).to(device))\n",
    "                epoch_total_loss += batch_loss.item()\n",
    "\n",
    "                if split == \"Train\":\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            avrg_loss = epoch_total_loss / loader.dataset.__len__()\n",
    "            accuracy = metrics.accuracy_score(labels, predictions)\n",
    "            confusion = metrics.confusion_matrix(labels, predictions)\n",
    "            _f1_score = f1_score(labels, predictions, average=\"macro\")\n",
    "            auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
    "            print(\"%s Epoch %d - loss=%0.4f AUC=%0.4f F1=%0.4f  Accuracy=%0.4f\" % (split, epoch, avrg_loss, auc, _f1_score, accuracy))\n",
    "\n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'best_f1': best_f1,\n",
    "            'f1': _f1_score,\n",
    "            'auc': auc,\n",
    "            'loss': avrg_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'opt_dict': optimizer.state_dict(),\n",
    "            #'scheduler_dict': scheduler.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(output_dir, f\"checkpoint_{epoch}.pt\"))\n",
    "        if _f1_score > best_f1:\n",
    "            best_f1 = _f1_score\n",
    "            checkpoint[\"best_f1\"] = best_f1\n",
    "            torch.save(checkpoint, os.path.join(output_dir, \"best.pt\"))\n",
    "else:\n",
    "    print(\"Skipping training\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d143e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"./{output_dir}/CKPT_TO_TEST.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Best F1 {} from epoch {}\\n\".format(checkpoint[\"best_f1\"], checkpoint[\"epoch\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e62fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "labels = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (inp, target) in tqdm(test_loader):\n",
    "        labels+=(target)\n",
    "        batch_prediction = model(inp.to(device))\n",
    "        _, batch_prediction = torch.max(batch_prediction, dim=1)\n",
    "        predictions += batch_prediction.detach().tolist()\n",
    "accuracy = metrics.accuracy_score(labels, predictions)\n",
    "print(\"Test Accuracy = %0.5f\" % (accuracy))\n",
    "\n",
    "confusion = metrics.confusion_matrix(labels, predictions)\n",
    "print(confusion)\n",
    "\n",
    "_f1_score = f1_score(labels, predictions, average=\"macro\")\n",
    "print(\"Test F1 = %0.5f\" % (_f1_score))\n",
    "\n",
    "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
    "print(\"Test AUC = %0.5f\" % (auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
