{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec31a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc003fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s):\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    os.environ['PYTHONHASHSEED'] = str(s)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0993a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import equalize_adapthist\n",
    "from skimage.transform import warp_polar\n",
    "\n",
    "class CLAHE(torch.nn.Module):\n",
    "    def forward(self, img):\n",
    "        image = np.array(img, dtype=np.float64) / 255.0\n",
    "        image = equalize_adapthist(image)\n",
    "        image = (image*255).astype('uint8')\n",
    "\n",
    "        return image\n",
    "\n",
    "class POLAR(torch.nn.Module):\n",
    "    def polar(self,image):\n",
    "        return warp_polar(image, radius=(max(image.shape) // 2), multichannel=True)\n",
    "    \n",
    "    def forward(self, image):\n",
    "        image = np.array(image, dtype=np.float64)\n",
    "        image = self.polar(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f297cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "split = \"test\"\n",
    "batch_size = 32\n",
    "num_workers = 32\n",
    "train_path = f\"/l/users/20020052/data/rim_one_dl/partitioned_by_hospital/training_set\" # path to dataset training set\n",
    "path = f\"/l/users/20020052/data/rim_one_dl/partitioned_by_hospital/{split}_set\"        # path to dataset folder\n",
    "output_dir = \"./output_rim_one_dl_final\"                                               # path to save checkpoints\n",
    "\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "            CLAHE(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomAffine(0,scale=(1.0,1.3))\n",
    "        ])\n",
    "transform = torchvision.transforms.Compose([\n",
    "            CLAHE(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(256)\n",
    "        ])\n",
    "train_dataset = ImageFolder(train_path, transform=train_transform)\n",
    "num = int(np.floor(len(train_dataset) * 1))\n",
    "indices = np.random.choice(len(train_dataset), num, replace=False)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                  batch_size=batch_size, \n",
    "                  shuffle=True,\n",
    "                  num_workers=num_workers,\n",
    "              )\n",
    "test_dataset = ImageFolder(path, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                  batch_size=batch_size, \n",
    "                  shuffle=True,\n",
    "                  num_workers=num_workers,\n",
    "              )\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0725981d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_labels = []\n",
    "for j in range(len(train_dataset)):\n",
    "    _labels.append(train_dataset[j][1])\n",
    "_labels = np.asarray(_labels)\n",
    "np.unique(_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18daca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model_name = \"efficientnet_b0\"\n",
    "pretrained = True\n",
    "dropout = 0.2\n",
    "lr = 0.0005\n",
    "#momentum = 0.1\n",
    "epochs = 20\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = timm.create_model(model_name, pretrained=pretrained, num_classes=2, drop_rate=dropout)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350aaa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 0.7765282022856348 from epoch 27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"/l/users/20020052/best_64_1.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Best F1 {} from epoch {}\\n\".format(checkpoint[\"best_f1\"], checkpoint[\"epoch\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd683741",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7974359 1.3405173]\n",
      "Resuming training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 - loss=0.0444 AUC=0.6355 F1=0.6294  Accuracy=0.7203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:25<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 - loss=0.0223 AUC=0.7501 F1=0.7670  Accuracy=0.8071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 - loss=0.0122 AUC=0.8254 F1=0.8337  Accuracy=0.8489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:26<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 3 - loss=0.0080 AUC=0.9107 F1=0.9050  Accuracy=0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:25<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 4 - loss=0.0072 AUC=0.9057 F1=0.8930  Accuracy=0.8971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:26<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 5 - loss=0.0074 AUC=0.8989 F1=0.8863  Accuracy=0.8907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 6 - loss=0.0058 AUC=0.9323 F1=0.9226  Accuracy=0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:25<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 7 - loss=0.0064 AUC=0.9288 F1=0.9221  Accuracy=0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:26<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 8 - loss=0.0066 AUC=0.9082 F1=0.9018  Accuracy=0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:26<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 9 - loss=0.0058 AUC=0.9271 F1=0.9219  Accuracy=0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:21<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 10 - loss=0.0052 AUC=0.9400 F1=0.9324  Accuracy=0.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 11 - loss=0.0040 AUC=0.9502 F1=0.9456  Accuracy=0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 12 - loss=0.0039 AUC=0.9546 F1=0.9490  Accuracy=0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:21<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 13 - loss=0.0036 AUC=0.9554 F1=0.9522  Accuracy=0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:25<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 14 - loss=0.0033 AUC=0.9580 F1=0.9493  Accuracy=0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 15 - loss=0.0036 AUC=0.9443 F1=0.9358  Accuracy=0.9389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:24<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 16 - loss=0.0028 AUC=0.9837 F1=0.9828  Accuracy=0.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:20<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 17 - loss=0.0032 AUC=0.9554 F1=0.9522  Accuracy=0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:26<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 18 - loss=0.0037 AUC=0.9589 F1=0.9525  Accuracy=0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:21<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 19 - loss=0.0030 AUC=0.9605 F1=0.9589  Accuracy=0.9614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.utils import class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "weight_referable = class_weight.compute_class_weight(class_weight='balanced', classes = np.unique(_labels), y=_labels).astype('float32')    \n",
    "weight_referable = np.array([weight_referable[0], weight_referable[1]])\n",
    "criterion = CrossEntropyLoss(weight=torch.from_numpy(weight_referable).to(device))\n",
    "print(weight_referable)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "epoch_resume = 0\n",
    "best_f1 = 0.0\n",
    "\n",
    "\n",
    "# Train\n",
    "if epoch_resume < epochs:\n",
    "    print(\"Resuming training\\n\")\n",
    "    for epoch in range(epoch_resume, epochs):\n",
    "        for split in ['Train']:\n",
    "            if split == \"Train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_total_loss = 0\n",
    "            labels = []\n",
    "            predictions = []\n",
    "            loader = train_loader if split == \"Train\" else val_loader\n",
    "            for batch_num, (inp, target) in enumerate(tqdm(loader)):\n",
    "                labels+=(target)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inp.to(device))\n",
    "                _, batch_prediction = torch.max(output, dim=1)\n",
    "                predictions += batch_prediction.detach().tolist()\n",
    "                batch_loss = criterion(output, (target).to(device))\n",
    "                epoch_total_loss += batch_loss.item()\n",
    "\n",
    "                if split == \"Train\":\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            avrg_loss = epoch_total_loss / loader.dataset.__len__()\n",
    "            accuracy = metrics.accuracy_score(labels, predictions)\n",
    "            confusion = metrics.confusion_matrix(labels, predictions)\n",
    "            _f1_score = f1_score(labels, predictions, average=\"macro\")\n",
    "            auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
    "            print(\"%s Epoch %d - loss=%0.4f AUC=%0.4f F1=%0.4f  Accuracy=%0.4f\" % (split, epoch, avrg_loss, auc, _f1_score, accuracy))\n",
    "\n",
    "\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'best_f1': best_f1,\n",
    "            'f1': _f1_score,\n",
    "            'auc': auc,\n",
    "            'loss': avrg_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'opt_dict': optimizer.state_dict(),\n",
    "            #'scheduler_dict': scheduler.state_dict()\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(output_dir, f\"checkpoint_{epoch}.pt\"))\n",
    "        if _f1_score > best_f1:\n",
    "            best_f1 = _f1_score\n",
    "            checkpoint[\"best_f1\"] = best_f1\n",
    "            torch.save(checkpoint, os.path.join(output_dir, \"best.pt\"))\n",
    "else:\n",
    "    print(\"Skipping training\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d143e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 0.9490370014311152 from epoch 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = f\"./{output_dir}/checkpoint_13.pt\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"Best F1 {} from epoch {}\\n\".format(checkpoint[\"best_f1\"], checkpoint[\"epoch\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e62fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [00:09<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.90805\n",
      "[[107  11]\n",
      " [  5  51]]\n",
      "Test F1 = 0.89742\n",
      "Test AUC = 0.90875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "labels = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (inp, target) in tqdm(test_loader):\n",
    "        labels+=(target)\n",
    "        batch_prediction = model(inp.to(device))\n",
    "        _, batch_prediction = torch.max(batch_prediction, dim=1)\n",
    "        predictions += batch_prediction.detach().tolist()\n",
    "accuracy = metrics.accuracy_score(labels, predictions)\n",
    "print(\"Test Accuracy = %0.5f\" % (accuracy))\n",
    "\n",
    "confusion = metrics.confusion_matrix(labels, predictions)\n",
    "print(confusion)\n",
    "\n",
    "_f1_score = f1_score(labels, predictions, average=\"macro\")\n",
    "print(\"Test F1 = %0.5f\" % (_f1_score))\n",
    "\n",
    "auc = sklearn.metrics.roc_auc_score(labels, predictions)\n",
    "print(\"Test AUC = %0.5f\" % (auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
